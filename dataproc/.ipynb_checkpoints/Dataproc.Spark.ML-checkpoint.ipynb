{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f459cc1b",
   "metadata": {},
   "source": [
    "### Machine learning by Spark in Dataproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546376f",
   "metadata": {},
   "source": [
    "from https://www.cloudskillsboost.google/focuses/3390?locale=en&parent=catalog\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a12a8",
   "metadata": {},
   "source": [
    "Apache Spark is an analytics engine for large scale data processing. Logistic regression is available as a module in Apache Spark's machine learning library, MLlib. Spark MLlib, also called Spark ML, includes implementations for most standard machine learning algorithms such as k-means clustering, random forests, alternating least squares, k-means clustering, decision trees, support vector machines, etc. Spark can run on a Hadoop cluster, like Dataproc, in order to process very large datasets in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5205c",
   "metadata": {},
   "source": [
    "Note: you need to create a storage bucket \"my-project-1011-1012-dsongcp\" as this was not mentioned in the guide book.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acfba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78570e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT=PROJECT[0]\n",
    "BUCKET = PROJECT + '-dsongcp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca43dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = PROJECT + '-dsongcp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c31a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local', 'logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Logistic regression w/ Spark ML') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72652370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe120a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('gs://{}/spark/sample_svm_data.txt'.format(BUCKET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a431ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = data.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b0712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
